{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: /Users/brad/Desktop/Plant_Pal/Modeling\n",
      "Data Directory: /Users/brad/Desktop/Plant_Pal/Modeling/../Data\n"
     ]
    }
   ],
   "source": [
    "base_directory = os.getcwd()\n",
    "data_directory = base_directory + \"/../Data\"\n",
    "print(\"Base Directory:\", base_directory)\n",
    "print(\"Data Directory:\", data_directory)\n",
    "\n",
    "data_dirs = os.listdir(data_directory)\n",
    "\n",
    "# Add the sibling directory to sys.path\n",
    "util_dir = os.path.join(base_directory, '..', 'Utility')\n",
    "sys.path.append(util_dir)\n",
    "\n",
    "from helper import disp_image   # test sibling directory import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label_dict = {}\n",
    "\n",
    "for ind, data_dir in enumerate(sorted(data_dirs)):\n",
    "    if data_dir == \".DS_Store\":\n",
    "        continue\n",
    "    label_dict[data_dir] = ind\n",
    "    species, label = data_dir.split('___')  # how species and label are delineated in filenames\n",
    "    image_paths = os.listdir(os.path.join(data_directory, data_dir))\n",
    "    for image in image_paths:\n",
    "        entry = [species, label, os.path.join(data_dir, image)]\n",
    "        data.append(entry)\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['Species', 'Condition', 'Image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 77836\n",
      "Validation set size: 9730\n",
      "Test set size: 9730\n",
      "Adjusted Training set size: 11052\n",
      "(tensor([[[ 0.1939,  0.2967, -0.0972,  ...,  0.5707,  0.5022,  0.2796],\n",
      "         [-0.3369,  0.0398,  0.5707,  ...,  0.5364,  0.5707,  0.4166],\n",
      "         [-0.3883,  0.0569, -0.3027,  ...,  0.4508,  0.5878,  0.5536],\n",
      "         ...,\n",
      "         [-0.2856, -0.3541, -0.1828,  ...,  0.8447,  0.7933,  0.7248],\n",
      "         [-0.0116,  0.3481,  0.0056,  ...,  0.7419,  0.6563,  0.6049],\n",
      "         [-0.0629,  0.4337,  0.0912,  ...,  0.7591,  0.6392,  0.5364]],\n",
      "\n",
      "        [[ 0.2577,  0.3627, -0.0399,  ...,  0.6779,  0.6078,  0.3803],\n",
      "         [-0.2850,  0.1001,  0.6429,  ...,  0.6429,  0.6779,  0.5203],\n",
      "         [-0.3375,  0.1176, -0.2500,  ...,  0.5553,  0.6954,  0.6604],\n",
      "         ...,\n",
      "         [-0.2325, -0.3025, -0.1275,  ...,  0.9580,  0.9055,  0.8354],\n",
      "         [ 0.0476,  0.4153,  0.0651,  ...,  0.8529,  0.7654,  0.7129],\n",
      "         [-0.0049,  0.5028,  0.1527,  ...,  0.8704,  0.7479,  0.6429]],\n",
      "\n",
      "        [[ 0.4614,  0.5659,  0.1651,  ...,  0.9145,  0.8448,  0.6182],\n",
      "         [-0.0790,  0.3045,  0.8448,  ...,  0.8797,  0.9145,  0.7576],\n",
      "         [-0.1312,  0.3219, -0.0441,  ...,  0.7925,  0.9319,  0.8971],\n",
      "         ...,\n",
      "         [ 0.0082, -0.0615,  0.1128,  ...,  1.1934,  1.1411,  1.0714],\n",
      "         [ 0.2871,  0.6531,  0.3045,  ...,  1.0888,  1.0017,  0.9494],\n",
      "         [ 0.2348,  0.7402,  0.3916,  ...,  1.1062,  0.9842,  0.8797]]]), 21)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train (80%) and temp (20%)\n",
    "train_df, temp_df = train_test_split(data_df, test_size=0.2, stratify=data_df[['Species', 'Condition']], random_state=2)\n",
    "\n",
    "# Split the temp data into validation (10%) and test (10%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[['Species', 'Condition']], random_state=2)\n",
    "\n",
    "print(\"Training set size:\", len(train_df))\n",
    "print(\"Validation set size:\", len(val_df))\n",
    "print(\"Test set size:\", len(test_df))\n",
    "# Adjust the training dataframe to ensure no more than 300 samples of any particular condition\n",
    "train_df = train_df.groupby('Condition').apply(lambda x: x.sample(n=min(len(x), 300), random_state=2)).reset_index(drop=True)\n",
    "\n",
    "print(\"Adjusted Training set size:\", len(train_df))\n",
    "\n",
    "train_set = CustomDataset(train_df['Image'])\n",
    "val_set = CustomDataset(val_df['Image'])\n",
    "test_set = CustomDataset(test_df['Image'])\n",
    "\n",
    "print(train_set.__getitem__(1))\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs=10, learning_rate=0.001):\n",
    "    early_stopper = Historian(early_stopping=3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'----- Epoch {epoch + 1}/{num_epochs} -----')\n",
    "        train_preds, val_preds = [0, 0], [0, 0] # (correct, total)\n",
    "        running_tl, running_vl = 0.0, 0.0\n",
    "        # --- Training ---\n",
    "        for data in tqdm(training_loader, desc='Training Batches'):\n",
    "            inputs = data[0].to(device)\n",
    "            test1 = data[0]\n",
    "            test2 = data[1]\n",
    "            labels = data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            train_loss = criterion(outputs, labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_tl += train_loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_preds[0] += (predicted == labels).sum().item()\n",
    "            train_preds[1] += labels.size(0)\n",
    "        # ------------------\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        # === Validation ===\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(validation_loader, desc='Validation Batches'):\n",
    "                inputs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "\n",
    "                running_vl += val_loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_preds[0] += (predicted == labels).sum().item()\n",
    "                val_preds[1] += labels.size(0)\n",
    "        # ==================\n",
    "        avg_tl, avg_ta = running_tl / len(training_loader), train_preds[0] / train_preds[1]\n",
    "        avg_vl, avg_va = running_vl / len(validation_loader), val_preds[0] / val_preds[1]\n",
    "        if not early_stopper.record(avg_tl, avg_ta, avg_vl, avg_va):\n",
    "            break\n",
    "        early_stopper.save_model()\n",
    "        print()\n",
    "    early_stopper.final_performance(verbose=True)\n",
    "    return running_tl / len(training_loader)\n",
    "\n",
    "def cross_validation(rates=[0.1, 0.000001]):\n",
    "    best_rate = None\n",
    "    best_loss = float('inf')\n",
    "    for rate in np.arange(rates[0], rates[1], -0.1):\n",
    "        loss = train(learning_rate=rate)\n",
    "        print(f'Learning rate: {rate}, Loss: {loss}')\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_rate = rate\n",
    "    print(f'Best learning rate: {best_rate}, Best loss: {best_loss}')\n",
    "    return best_rate, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for inference\n",
      "----- Epoch 1/10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:  19%|█▉        | 134/691 [11:29<1:01:52,  6.66s/it]"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')\n",
    "model = torchvision.models.resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "train(model, num_epochs=10, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
